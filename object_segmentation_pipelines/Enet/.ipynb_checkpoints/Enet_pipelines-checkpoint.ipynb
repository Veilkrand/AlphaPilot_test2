{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Scene Understanding using Deep Learning\n",
    "## Introduction\n",
    "\n",
    "This Notebook was written for demonstrating scene understanding for the Lockheed Martin drone challange. The code consists of three pipelines pre-processing, FCN and post processing  The project is a corner stone for  \n",
    "\n",
    "\\<img style=\"float: center;\" src=\"readme_imgs/Img_groundtruth.png\">\n",
    "\n",
    "## Data Pre-processing \n",
    "\n",
    "We have implemented camera calibration routine to the video file.Each image was normalized and then smoothed with a Gaussian filter. The images were randomly processed with a brightness filter to help the network generalize to different lighting conditions. \n",
    "\n",
    " \n",
    "\n",
    "\n",
    "## FCN\n",
    "\n",
    " \n",
    "\n",
    " \n",
    " \n",
    "\n",
    "##  Jaccard similarity coefficient \n",
    "\n",
    "In evaluating the model I've investigated several metrics including the Mean IU, Intersection over Union, and the Jaccard coefficient. The idea is to maximize the overlap between the predicted region and the ground truth bounding box.\n",
    " \n",
    "We eventually decided to use the Jaccard coeef. The Jaccard similarity coefficient is defined as the size of the intersection divided by the size of the union of two regions. This metric is used to compare the predicted labels to a set of labels in y_true  \n",
    "\n",
    "The coefficients are given by \n",
    "\n",
    "#### J(A,B) = |A∩B| / |A∪B|=|A∩B|/|A|+|B|-|A∩B| \n",
    "\n",
    "(If A and B are both empty, we define J(A,B) = 1.)\n",
    "\n",
    "<img style=\"float: center;\" src=\"readme_imgs/Intersection_of_sets_A_and_B.png\">\n",
    "<img style=\"float: center;\" src=\"readme_imgs/Intersection_of_sets_A_and_B_2.png\">\n",
    " \n",
    "\n",
    "## Training\n",
    "\n",
    "The Autti dataset was used and can be obtained from https://github.com/udacity/self-driving-car/tree/master/annotations .  \n",
    "\n",
    "## Results\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import json\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Rectangle\n",
    "#from moviepy.editor import VideoFileClip\n",
    "from IPython.display import HTML\n",
    "import numpy as np\n",
    "import glob\n",
    "import time\n",
    "\n",
    "from scipy.ndimage.measurements import label\n",
    "import pandas as pd\n",
    "#import keras.backend as K\n",
    "from sklearn.metrics import jaccard_similarity_score\n",
    " \n",
    "import time\n",
    "import numpy\n",
    "from PIL import Image, ImageDraw\n",
    "import re\n",
    "from shapely.geometry import Polygon\n",
    "from pprint import pprint\n",
    "import PIL.ImageDraw\n",
    "from PIL import Image\n",
    "import argparse\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    " \n",
    "#add a note for the class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading and processing the wkt json format \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df is the dataset that we are annotating\n",
    "#raw_df is the dataset that the organizers provided\n",
    "#adding path for json anf image folders respectively\n",
    "provided_data_file_dir='/media/a/D/lockheed-martin/dataset/LM_dataset/csv/training_GT_labels_v2.json'#json provided by the organizers of the challange\n",
    "our_data_file_dir='/media/a/D/lockheed-martin/dataset/LM_dataset/csv/data_wkt_v4.json'#our annotated dataset\n",
    "img_file_dir='/media/a/D/lockheed-martin/dataset/LM_dataset/Data_Training/'#folder where images are stored\n",
    "provided_df = pd.read_json(provided_data_file_dir)\n",
    "df = pd.read_json(our_data_file_dir)\n",
    "\n",
    " \n",
    "#adding a complete path for the image \n",
    "df['External ID']= [img_file_dir + u for u in df['External ID']]#iris_data_dir + new_df['parcel_id'].astype(str) + '.jpg'\n",
    "\n",
    "df['images']=[u.split('/',8)[8] for u in df['External ID']]\n",
    "raw_df=pd.DataFrame()\n",
    "raw_df['images']=list(provided_df.columns.values)\n",
    "raw_df['img_path']=[img_file_dir +  u for u in raw_df['images']]\n",
    "\n",
    "raw_df['raw_inner_poly']=[ provided_df[u][0] for u in raw_df['images']]\n",
    "\n",
    "raw_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove white spaces from image path\n",
    "s=raw_df['images'][3]\n",
    "print(re.sub(r\"\\s+\", \"\", s))  # \\s matches all white spaces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#remove white space from image name in the folder containing the images\n",
    "\n",
    "#go to img directory and run python fix.py. Here is the file\n",
    "\n",
    "#remove white space from image name in the folder containing the images\n",
    "import os\n",
    "import re\n",
    "\n",
    "\"\"\" \n",
    "Renames the filenames within the same directory to be Unix friendly\n",
    "(1) Changes spaces to nothing\n",
    "(2) Makes lowercase (not a Unix requirement, just looks better ;)\n",
    "Usage:\n",
    "python rename.py\n",
    "\"\"\"\n",
    "path =  os.getcwd()\n",
    " \n",
    "filenames = os.listdir(path)\n",
    "\n",
    "for filename in filenames:\n",
    "    print('img name befor',filename)\n",
    "    fixed_filename=re.sub(r\"\\s+\", \"\", filename)\n",
    "    print('img name after',fixed_filename)\n",
    "    os.rename(filename, fixed_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#merge the our annotations and provided annotations\n",
    "df_all=pd.merge(df, raw_df, on='images')\n",
    "df_all.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "keep_cols=['images', 'img_path','Label','raw_inner_poly']\n",
    "df_all=df_all[keep_cols]\n",
    "df_all['outer_poly']=''\n",
    "df_all['inner_poly']=''\n",
    "df_all['outer_x_min']=''\n",
    "df_all['outer_y_min']=''\n",
    "df_all['outer_x_max']=''\n",
    "df_all['outer_y_max']=''\n",
    "df_all['inner_x_min']=''\n",
    "df_all['inner_y_min']=''\n",
    "df_all['inner_x_max']=''\n",
    "df_all['inner_y_max']=''\n",
    "df_all['class_id']= ''\n",
    "print(len(df_all))\n",
    "df_all.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking the mask from the provided GT by the organizers\n",
    "\n",
    "\n",
    "\n",
    "#processing the polygone and creating a mask\n",
    "def get_mask_raw_data(img_shape, poly,display=False):\n",
    "    output_mask = np.zeros(img_shape[:2], dtype=np.uint8)\n",
    "    print('poly',poly)\n",
    "\n",
    "    coords =  zip(*[iter(poly)] * 2) \n",
    "     \n",
    "    mask = np.zeros(img_shape[:2], dtype=np.uint8)\n",
    "    mask = PIL.Image.fromarray(mask)\n",
    "    draw = PIL.ImageDraw.Draw(mask)\n",
    "    x = []\n",
    "    y = []\n",
    "    polygons = []\n",
    "\n",
    "    for pt in coords:\n",
    "        a = (int(pt[0]), int(pt[1]))\n",
    "        polygons.append(a)\n",
    "        init_x = (int(pt[0]))\n",
    "        init_y = (int(pt[1]))\n",
    "\n",
    "        x.append(init_x)\n",
    "        y.append(init_y)\n",
    "    proc_polygons=np.vstack((x,y)).T\n",
    "    x,y,w,h = cv2.boundingRect(proc_polygons) \n",
    "    draw.polygon(xy=polygons, outline=1, fill=1)\n",
    "    mask = np.array(mask, dtype=bool)\n",
    "    if display:\n",
    "        plt.imshow(mask)\n",
    "        plt.show\n",
    "    return mask\n",
    "for i in range(444,446):#len(df)):\n",
    "\n",
    "    img=cv2.imread(df_all['img_path'][i])\n",
    "    row=img.shape[0]\n",
    "    col=img.shape[1]\n",
    "    img_shape = (row,col)\n",
    "    #outer_poly=df['Label'][i] \n",
    "    inner_poly=df_all['raw_inner_poly'][i]\n",
    "    if inner_poly:\n",
    "        print('raw_inner_poly',inner_poly)\n",
    "    #outer_mask=get_mask(img_shape,outer_poly,display=False)\n",
    "        inner_mask=get_mask_raw_data(img_shape,inner_poly,display=False)\n",
    "        inner_mask.dtype='uint8'\n",
    "    #outer_mask.dtype='uint8'\n",
    "    #final_mask=cv2.subtract(outer_mask,inner_mask)\n",
    "        plt.imshow(cv2.bitwise_and(img,img,mask=inner_mask))\n",
    "        plt.show\n",
    "    else:\n",
    "        print('polygon data corruption detected for index:', i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test_id=1\n",
    "test_poly=df['Label'][test_id]['Outer Border']#df['outer_poly'][1] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#convert the polygons into a format that can be convereted to bounding boxes \n",
    "from tqdm import tqdm\n",
    "#preprocessing our_dataset\n",
    "\n",
    "def convert_coordinates(poly):\n",
    "    proc_poly=poly[0]['geometry']\n",
    "    nums =  re.findall(r'\\d+(?:\\.\\d*)?', proc_poly.rpartition(',')[0])\n",
    "    coords =  zip(*[iter(nums)] * 2)\n",
    "    polygons = []\n",
    "    for pt in coords:\n",
    "        a = (int(pt[0]), int(pt[1]))\n",
    "        polygons.append(a)\n",
    "    \n",
    "    return polygons \n",
    "def convert_coordinates_raw(polygon):\n",
    "    poly=[]\n",
    "    polygon =  zip(*[iter(polygon)] * 2)\n",
    "    for p in polygon:\n",
    "        poly.append(p)\n",
    "    return poly\n",
    "def get_bbox(polygon):\n",
    "    polygon=polygon[0]['geometry']\n",
    "    polygon=(polygon)\n",
    "\n",
    "    polygon =  re.findall(r'\\d+(?:\\.\\d*)?', polygon)\n",
    "    polygon =  zip(*[iter(polygon)] * 2)\n",
    "    x = []\n",
    "    y = []\n",
    "    for pt in polygon:\n",
    "        init_x = (int(pt[0]))\n",
    "        init_y = (int(pt[1]))\n",
    "\n",
    "        x.append(init_x)\n",
    "        y.append(init_y)\n",
    "    polygons=np.vstack((x,y)).T\n",
    "   \n",
    "    x,y,w,h = cv2.boundingRect(polygons)\n",
    "    x_min=x\n",
    "    y_min=y\n",
    "    x_max=x+w\n",
    "    y_max=y+h\n",
    "    return x_min,y_min,x_max,y_max\n",
    "    #return x_min,x_max,y_min,y_max\n",
    "def get_bbox_raw_data(polygon):\n",
    "    polygon =  zip(*[iter(polygon)] * 2)\n",
    "    x = []\n",
    "    y = []\n",
    "    for pt in polygon:\n",
    " \n",
    "        init_x = (int(pt[0]))\n",
    "        init_y = (int(pt[1]))\n",
    "\n",
    "        x.append(init_x)\n",
    "        y.append(init_y)\n",
    "    polygons=np.vstack(((x,y))).T\n",
    "    \n",
    "    x,y,w,h = cv2.boundingRect(polygons)\n",
    "    x_min=x\n",
    "    y_min=y\n",
    "    x_max=x+w\n",
    "    y_max=y+h\n",
    "    return x_min,y_min,x_max,y_max\n",
    "    #return x_min,x_max,y_min,y_max\n",
    "for i in tqdm(range(0,len(df_all))):\n",
    "    #remove white spaces from image path\n",
    "   \n",
    "    df_all['img_path'][i]=re.sub(r\"\\s+\", \"\", df_all['img_path'][i]) # \\s matches all white spaces\n",
    "    if df_all['raw_inner_poly'][i]:\n",
    "        if df_all['Label'][i] != 'Skip':\n",
    "            outer_poly=df_all['Label'][i]['Outer Border']\n",
    "            #if len(outer_poly) !=1:\n",
    " \n",
    "            inner_poly=df_all['raw_inner_poly'][i]#df['Label'][i]['inner flyable area']\n",
    "            if inner_poly and outer_poly:\n",
    "                df_all['outer_poly'][i] = convert_coordinates(outer_poly)\n",
    "                df_all['inner_poly'][i] = convert_coordinates_raw(inner_poly)\n",
    "                    #df['inner_poly'][i] = convert_coordinates(inner_poly)\n",
    "\n",
    "                df_all['outer_x_min'][i],df_all['outer_y_min'][i], df_all['outer_x_max'][i],df_all['outer_y_max'][i]=get_bbox(outer_poly)\n",
    "                df_all['inner_x_min'][i],df_all['inner_y_min'][i], df_all['inner_x_max'][i],df_all['inner_y_max'][i]=get_bbox_raw_data(inner_poly)\n",
    "                    #df_all['outer_x_min'][i],df_all['outer_x_max'][i], df_all['outer_y_min'][i],df_all['outer_y_max'][i]=get_bbox(outer_poly)\n",
    "                    #df_all['inner_x_min'][i],df_all['inner_x_max'][i], df_all['inner_y_min'][i],df_all['inner_y_max'][i]=get_bbox_raw_data(inner_poly) \n",
    "        else:\n",
    "            print('corrupt data detected for index:', i )\n",
    "            continue\n",
    "    #df['outer_poly']= h\n",
    "   #df['inner_poly']= convert_coordinates(inner_poly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all.reset_index(drop=True, inplace=True)\n",
    "df_all.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#keep only filed required by yolov3. Divide the dataframe into outer and inner bounding boxes and then assign a class to each one of them \n",
    "keep_cols_outer=['img_path', 'outer_x_min', 'outer_y_min', 'outer_x_max', 'outer_y_max','outer_poly','class_id']\n",
    "keep_cols_inner=['img_path', 'inner_x_min', 'inner_y_min', 'inner_x_max', 'inner_y_max', 'inner_poly','class_id']\n",
    "df_outer=df_all[keep_cols_outer]\n",
    "df_inner=df_all[keep_cols_inner]\n",
    "df_outer['class_id']=0\n",
    "df_inner['class_id']=1\n",
    "df_enet=pd.merge(df_outer, df_inner, on='img_path')\n",
    "df_enet.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing annotation all together \n",
    "\n",
    "def get_mask(img_shape, poly,display=False):\n",
    "     \n",
    "\n",
    "    mask = np.zeros(img_shape[:2], dtype=np.uint8)\n",
    "    mask = PIL.Image.fromarray(mask)\n",
    "    draw = PIL.ImageDraw.Draw(mask)\n",
    "    polygons = []\n",
    "    for pt in poly:\n",
    "        a = (int(pt[0]), int(pt[1]))\n",
    "        polygons.append(a)\n",
    "    \n",
    "    draw.polygon(xy=polygons, outline=1, fill=1)\n",
    "    mask = np.array(mask, dtype=bool)\n",
    "    if display:\n",
    "        plt.imshow(mask)\n",
    "        plt.show\n",
    "    return mask\n",
    "#getting the final mask\n",
    "\n",
    "for i in tqdm(range(0,len(df_enet))): # use one image only for testing\n",
    "    print(i)\n",
    "    img=cv2.imread(df_enet['img_path'][i])\n",
    "    row=img.shape[0]\n",
    "    col=img.shape[1]\n",
    "    img_shape = (row,col)\n",
    "    outer_poly=df_enet['outer_poly'][i] \n",
    "     \n",
    "    inner_poly=df_enet['inner_poly'][i]\n",
    "    print('outer_poly',outer_poly)\n",
    "    if len(outer_poly) < 2:\n",
    "        print('gt annotation error has been detected in img ID:', i)\n",
    "        print('skipping img')\n",
    "        \n",
    "    else: \n",
    "        outer_mask=get_mask(img_shape,outer_poly,display=False)\n",
    "        inner_mask=get_mask(img_shape,inner_poly,display=False)\n",
    "        inner_mask.dtype='uint8'\n",
    "        outer_mask.dtype='uint8'\n",
    "        final_mask=cv2.subtract(outer_mask,inner_mask)\n",
    "        plt.imshow(cv2.bitwise_and(img,img,mask=final_mask))\n",
    "        plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(df_enet))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the ENet model\n",
    "We decided to to split the model to three sub classes:\n",
    "\n",
    "1) Initial block\n",
    "\n",
    "2) RDDNeck - class for regular, downsampling and dilated bottlenecks\n",
    "\n",
    "3) ASNeck - class for asymetric bottlenecks\n",
    "\n",
    "4) UBNeck - class for upsampling bottlenecks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InitialBlock(nn.Module):\n",
    "  \n",
    "  # Initial block of the model:\n",
    "  #         Input\n",
    "  #        /     \\\n",
    "  #       /       \\\n",
    "  #maxpool2d    conv2d-3x3\n",
    "  #       \\       /  \n",
    "  #        \\     /\n",
    "  #      concatenate\n",
    "   \n",
    "    def __init__ (self,in_channels = 3,out_channels = 13):\n",
    "        super().__init__()\n",
    "\n",
    "\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=2, \n",
    "                                      stride = 2, \n",
    "                                      padding = 0)\n",
    "\n",
    "        self.conv = nn.Conv2d(in_channels, \n",
    "                                out_channels,\n",
    "                                kernel_size = 3,\n",
    "                                stride = 2, \n",
    "                                padding = 1)\n",
    "\n",
    "        self.prelu = nn.PReLU(16)\n",
    "\n",
    "        self.batchnorm = nn.BatchNorm2d(out_channels)\n",
    "  \n",
    "    def forward(self, x):\n",
    "        \n",
    "        main = self.conv(x)\n",
    "        main = self.batchnorm(main)\n",
    "        \n",
    "        side = self.maxpool(x)\n",
    "        \n",
    "        # concatenating on the channels axis\n",
    "        x = torch.cat((main, side), dim=1)\n",
    "        x = self.prelu(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UBNeck(nn.Module):\n",
    "    \n",
    "  # Upsampling bottleneck:\n",
    "  #     Bottleneck Input\n",
    "  #        /        \\\n",
    "  #       /          \\\n",
    "  # conv2d-1x1     convTrans2d-1x1\n",
    "  #      |             | PReLU\n",
    "  #      |         convTrans2d-3x3\n",
    "  #      |             | PReLU\n",
    "  #      |         convTrans2d-1x1\n",
    "  #      |             |\n",
    "  # maxunpool2d    Regularizer\n",
    "  #       \\           /  \n",
    "  #        \\         /\n",
    "  #      Summing + PReLU\n",
    "  #\n",
    "  #  Params: \n",
    "  #  projection_ratio - ratio between input and output channels\n",
    "  #  relu - if True: relu used as the activation function else: Prelu us used\n",
    "  \n",
    "    def __init__(self, in_channels, out_channels, relu=False, projection_ratio=4):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        # Define class variables\n",
    "        self.in_channels = in_channels\n",
    "        self.reduced_depth = int(in_channels / projection_ratio)\n",
    "        self.out_channels = out_channels\n",
    "        \n",
    "        \n",
    "        if relu:\n",
    "            activation = nn.ReLU()\n",
    "        else:\n",
    "            activation = nn.PReLU()\n",
    "        \n",
    "        self.unpool = nn.MaxUnpool2d(kernel_size = 2,\n",
    "                                     stride = 2)\n",
    "        \n",
    "        self.main_conv = nn.Conv2d(in_channels = self.in_channels,\n",
    "                                    out_channels = self.out_channels,\n",
    "                                    kernel_size = 1)\n",
    "        \n",
    "        self.dropout = nn.Dropout2d(p=0.1)\n",
    "        \n",
    "        \n",
    "        self.convt1 = nn.ConvTranspose2d(in_channels = self.in_channels,\n",
    "                               out_channels = self.reduced_depth,\n",
    "                               kernel_size = 1,\n",
    "                               padding = 0,\n",
    "                               bias = False)\n",
    "        \n",
    "        \n",
    "        self.prelu1 = activation\n",
    "        \n",
    "        # This layer used for Upsampling\n",
    "        self.convt2 = nn.ConvTranspose2d(in_channels = self.reduced_depth,\n",
    "                                  out_channels = self.reduced_depth,\n",
    "                                  kernel_size = 3,\n",
    "                                  stride = 2,\n",
    "                                  padding = 1,\n",
    "                                  output_padding = 1,\n",
    "                                  bias = False)\n",
    "        \n",
    "        self.prelu2 = activation\n",
    "        \n",
    "        self.convt3 = nn.ConvTranspose2d(in_channels = self.reduced_depth,\n",
    "                                  out_channels = self.out_channels,\n",
    "                                  kernel_size = 1,\n",
    "                                  padding = 0,\n",
    "                                  bias = False)\n",
    "        \n",
    "        self.prelu3 = activation\n",
    "        \n",
    "        self.batchnorm = nn.BatchNorm2d(self.reduced_depth)\n",
    "        self.batchnorm2 = nn.BatchNorm2d(self.out_channels)\n",
    "        \n",
    "    def forward(self, x, indices):\n",
    "        x_copy = x\n",
    "        \n",
    "        # Side Branch\n",
    "        x = self.convt1(x)\n",
    "        x = self.batchnorm(x)\n",
    "        x = self.prelu1(x)\n",
    "        \n",
    "        x = self.convt2(x)\n",
    "        x = self.batchnorm(x)\n",
    "        x = self.prelu2(x)\n",
    "        \n",
    "        x = self.convt3(x)\n",
    "        x = self.batchnorm2(x)\n",
    "        \n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # Main Branch\n",
    "        \n",
    "        x_copy = self.main_conv(x_copy)\n",
    "        x_copy = self.unpool(x_copy, indices, output_size=x.size())\n",
    "        \n",
    "        # summing the main and side branches\n",
    "        x = x + x_copy\n",
    "        x = self.prelu3(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RDDNeck(nn.Module):\n",
    "    def __init__(self, dilation, in_channels, out_channels, down_flag, relu=False, projection_ratio=4, p=0.1):\n",
    "      \n",
    "  # Regular|Dilated|Downsampling bottlenecks:\n",
    "  #\n",
    "  #     Bottleneck Input\n",
    "  #        /        \\\n",
    "  #       /          \\\n",
    "  # maxpooling2d   conv2d-1x1\n",
    "  #      |             | PReLU\n",
    "  #      |         conv2d-3x3\n",
    "  #      |             | PReLU\n",
    "  #      |         conv2d-1x1\n",
    "  #      |             |\n",
    "  #  Padding2d     Regularizer\n",
    "  #       \\           /  \n",
    "  #        \\         /\n",
    "  #      Summing + PReLU\n",
    "  #\n",
    "  # Params: \n",
    "  #  dilation (bool) - if True: creating dilation bottleneck\n",
    "  #  down_flag (bool) - if True: creating downsampling bottleneck\n",
    "  #  projection_ratio - ratio between input and output channels\n",
    "  #  relu - if True: relu used as the activation function else: Prelu us used\n",
    "  #  p - dropout ratio\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        # Define class variables\n",
    "        self.in_channels = in_channels\n",
    "        \n",
    "        self.out_channels = out_channels\n",
    "        self.dilation = dilation\n",
    "        self.down_flag = down_flag\n",
    "        \n",
    "        # calculating the number of reduced channels\n",
    "        if down_flag:\n",
    "            self.stride = 2\n",
    "            self.reduced_depth = int(in_channels // projection_ratio)\n",
    "        else:\n",
    "            self.stride = 1\n",
    "            self.reduced_depth = int(out_channels // projection_ratio)\n",
    "        \n",
    "        if relu:\n",
    "            activation = nn.ReLU()\n",
    "        else:\n",
    "            activation = nn.PReLU()\n",
    "        \n",
    "        self.maxpool = nn.MaxPool2d(kernel_size = 2,\n",
    "                                      stride = 2,\n",
    "                                      padding = 0, return_indices=True)\n",
    "        \n",
    "\n",
    "        \n",
    "        self.dropout = nn.Dropout2d(p=p)\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channels = self.in_channels,\n",
    "                               out_channels = self.reduced_depth,\n",
    "                               kernel_size = 1,\n",
    "                               stride = 1,\n",
    "                               padding = 0,\n",
    "                               bias = False,\n",
    "                               dilation = 1)\n",
    "        \n",
    "        self.prelu1 = activation\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(in_channels = self.reduced_depth,\n",
    "                                  out_channels = self.reduced_depth,\n",
    "                                  kernel_size = 3,\n",
    "                                  stride = self.stride,\n",
    "                                  padding = self.dilation,\n",
    "                                  bias = True,\n",
    "                                  dilation = self.dilation)\n",
    "                                  \n",
    "        self.prelu2 = activation\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(in_channels = self.reduced_depth,\n",
    "                                  out_channels = self.out_channels,\n",
    "                                  kernel_size = 1,\n",
    "                                  stride = 1,\n",
    "                                  padding = 0,\n",
    "                                  bias = False,\n",
    "                                  dilation = 1)\n",
    "        \n",
    "        self.prelu3 = activation\n",
    "        \n",
    "        self.batchnorm = nn.BatchNorm2d(self.reduced_depth)\n",
    "        self.batchnorm2 = nn.BatchNorm2d(self.out_channels)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        bs = x.size()[0]\n",
    "        x_copy = x\n",
    "        \n",
    "        # Side Branch\n",
    "        x = self.conv1(x)\n",
    "        x = self.batchnorm(x)\n",
    "        x = self.prelu1(x)\n",
    "        \n",
    "        x = self.conv2(x)\n",
    "        x = self.batchnorm(x)\n",
    "        x = self.prelu2(x)\n",
    "        \n",
    "        x = self.conv3(x)\n",
    "        x = self.batchnorm2(x)\n",
    "                \n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # Main Branch\n",
    "        if self.down_flag:\n",
    "            x_copy, indices = self.maxpool(x_copy)\n",
    "          \n",
    "        if self.in_channels != self.out_channels:\n",
    "            out_shape = self.out_channels - self.in_channels\n",
    "            \n",
    "            #padding and concatenating in order to match the channels axis of the side and main branches\n",
    "            extras = torch.zeros((bs, out_shape, x.shape[2], x.shape[3]))\n",
    "            if torch.cuda.is_available():\n",
    "                extras = extras.cuda()\n",
    "            x_copy = torch.cat((x_copy, extras), dim = 1)\n",
    "\n",
    "        # Summing main and side branches\n",
    "        x = x + x_copy\n",
    "        x = self.prelu3(x)\n",
    "        \n",
    "        if self.down_flag:\n",
    "            return x, indices\n",
    "        else:\n",
    "            return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ASNeck(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, projection_ratio=4):\n",
    "      \n",
    "  # Asymetric bottleneck:\n",
    "  #\n",
    "  #     Bottleneck Input\n",
    "  #        /        \\\n",
    "  #       /          \\\n",
    "  #      |         conv2d-1x1\n",
    "  #      |             | PReLU\n",
    "  #      |         conv2d-1x5\n",
    "  #      |             |\n",
    "  #      |         conv2d-5x1\n",
    "  #      |             | PReLU\n",
    "  #      |         conv2d-1x1\n",
    "  #      |             |\n",
    "  #  Padding2d     Regularizer\n",
    "  #       \\           /  \n",
    "  #        \\         /\n",
    "  #      Summing + PReLU\n",
    "  #\n",
    "  # Params:    \n",
    "  #  projection_ratio - ratio between input and output channels\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        # Define class variables\n",
    "        self.in_channels = in_channels\n",
    "        self.reduced_depth = int(in_channels / projection_ratio)\n",
    "        self.out_channels = out_channels\n",
    "        \n",
    "        self.dropout = nn.Dropout2d(p=0.1)\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(in_channels = self.in_channels,\n",
    "                               out_channels = self.reduced_depth,\n",
    "                               kernel_size = 1,\n",
    "                               stride = 1,\n",
    "                               padding = 0,\n",
    "                               bias = False)\n",
    "        \n",
    "        self.prelu1 = nn.PReLU()\n",
    "        \n",
    "        self.conv21 = nn.Conv2d(in_channels = self.reduced_depth,\n",
    "                                  out_channels = self.reduced_depth,\n",
    "                                  kernel_size = (1, 5),\n",
    "                                  stride = 1,\n",
    "                                  padding = (0, 2),\n",
    "                                  bias = False)\n",
    "        \n",
    "        self.conv22 = nn.Conv2d(in_channels = self.reduced_depth,\n",
    "                                  out_channels = self.reduced_depth,\n",
    "                                  kernel_size = (5, 1),\n",
    "                                  stride = 1,\n",
    "                                  padding = (2, 0),\n",
    "                                  bias = False)\n",
    "        \n",
    "        self.prelu2 = nn.PReLU()\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(in_channels = self.reduced_depth,\n",
    "                                  out_channels = self.out_channels,\n",
    "                                  kernel_size = 1,\n",
    "                                  stride = 1,\n",
    "                                  padding = 0,\n",
    "                                  bias = False)\n",
    "        \n",
    "        self.prelu3 = nn.PReLU()\n",
    "        \n",
    "        self.batchnorm = nn.BatchNorm2d(self.reduced_depth)\n",
    "        self.batchnorm2 = nn.BatchNorm2d(self.out_channels)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        bs = x.size()[0]\n",
    "        x_copy = x\n",
    "        \n",
    "        # Side Branch\n",
    "        x = self.conv1(x)\n",
    "        x = self.batchnorm(x)\n",
    "        x = self.prelu1(x)\n",
    "        \n",
    "        x = self.conv21(x)\n",
    "        x = self.conv22(x)\n",
    "        x = self.batchnorm(x)\n",
    "        x = self.prelu2(x)\n",
    "        \n",
    "        x = self.conv3(x)\n",
    "                \n",
    "        x = self.dropout(x)\n",
    "        x = self.batchnorm2(x)\n",
    "        \n",
    "        # Main Branch\n",
    "        \n",
    "        if self.in_channels != self.out_channels:\n",
    "            out_shape = self.out_channels - self.in_channels\n",
    "            \n",
    "            #padding and concatenating in order to match the channels axis of the side and main branches\n",
    "            extras = torch.zeros((bs, out_shape, x.shape[2], x.shape[3]))\n",
    "            if torch.cuda.is_available():\n",
    "                extras = extras.cuda()\n",
    "            x_copy = torch.cat((x_copy, extras), dim = 1)\n",
    "        \n",
    "        # Summing main and side branches\n",
    "        x = x + x_copy\n",
    "        x = self.prelu3(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ENet(nn.Module):\n",
    "  \n",
    "  # Creating Enet model!\n",
    "  \n",
    "    def __init__(self, C):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Define class variables\n",
    "        # C - number of classes\n",
    "        self.C = C\n",
    "        \n",
    "        # The initial block\n",
    "        self.init = InitialBlock()\n",
    "        \n",
    "        \n",
    "        # The first bottleneck\n",
    "        self.b10 = RDDNeck(dilation=1, \n",
    "                           in_channels=16, \n",
    "                           out_channels=64, \n",
    "                           down_flag=True, \n",
    "                           p=0.01)\n",
    "        \n",
    "        self.b11 = RDDNeck(dilation=1, \n",
    "                           in_channels=64, \n",
    "                           out_channels=64, \n",
    "                           down_flag=False, \n",
    "                           p=0.01)\n",
    "        \n",
    "        self.b12 = RDDNeck(dilation=1, \n",
    "                           in_channels=64, \n",
    "                           out_channels=64, \n",
    "                           down_flag=False, \n",
    "                           p=0.01)\n",
    "        \n",
    "        self.b13 = RDDNeck(dilation=1, \n",
    "                           in_channels=64, \n",
    "                           out_channels=64, \n",
    "                           down_flag=False, \n",
    "                           p=0.01)\n",
    "        \n",
    "        self.b14 = RDDNeck(dilation=1, \n",
    "                           in_channels=64, \n",
    "                           out_channels=64, \n",
    "                           down_flag=False, \n",
    "                           p=0.01)\n",
    "        \n",
    "        \n",
    "        # The second bottleneck\n",
    "        self.b20 = RDDNeck(dilation=1, \n",
    "                           in_channels=64, \n",
    "                           out_channels=128, \n",
    "                           down_flag=True)\n",
    "        \n",
    "        self.b21 = RDDNeck(dilation=1, \n",
    "                           in_channels=128, \n",
    "                           out_channels=128, \n",
    "                           down_flag=False)\n",
    "        \n",
    "        self.b22 = RDDNeck(dilation=2, \n",
    "                           in_channels=128, \n",
    "                           out_channels=128, \n",
    "                           down_flag=False)\n",
    "        \n",
    "        self.b23 = ASNeck(in_channels=128, \n",
    "                          out_channels=128)\n",
    "        \n",
    "        self.b24 = RDDNeck(dilation=4, \n",
    "                           in_channels=128, \n",
    "                           out_channels=128, \n",
    "                           down_flag=False)\n",
    "        \n",
    "        self.b25 = RDDNeck(dilation=1, \n",
    "                           in_channels=128, \n",
    "                           out_channels=128, \n",
    "                           down_flag=False)\n",
    "        \n",
    "        self.b26 = RDDNeck(dilation=8, \n",
    "                           in_channels=128, \n",
    "                           out_channels=128, \n",
    "                           down_flag=False)\n",
    "        \n",
    "        self.b27 = ASNeck(in_channels=128, \n",
    "                          out_channels=128)\n",
    "        \n",
    "        self.b28 = RDDNeck(dilation=16, \n",
    "                           in_channels=128, \n",
    "                           out_channels=128, \n",
    "                           down_flag=False)\n",
    "        \n",
    "        \n",
    "        # The third bottleneck\n",
    "        self.b31 = RDDNeck(dilation=1, \n",
    "                           in_channels=128, \n",
    "                           out_channels=128, \n",
    "                           down_flag=False)\n",
    "        \n",
    "        self.b32 = RDDNeck(dilation=2, \n",
    "                           in_channels=128, \n",
    "                           out_channels=128, \n",
    "                           down_flag=False)\n",
    "        \n",
    "        self.b33 = ASNeck(in_channels=128, \n",
    "                          out_channels=128)\n",
    "        \n",
    "        self.b34 = RDDNeck(dilation=4, \n",
    "                           in_channels=128, \n",
    "                           out_channels=128, \n",
    "                           down_flag=False)\n",
    "        \n",
    "        self.b35 = RDDNeck(dilation=1, \n",
    "                           in_channels=128, \n",
    "                           out_channels=128, \n",
    "                           down_flag=False)\n",
    "        \n",
    "        self.b36 = RDDNeck(dilation=8, \n",
    "                           in_channels=128, \n",
    "                           out_channels=128, \n",
    "                           down_flag=False)\n",
    "        \n",
    "        self.b37 = ASNeck(in_channels=128, \n",
    "                          out_channels=128)\n",
    "        \n",
    "        self.b38 = RDDNeck(dilation=16, \n",
    "                           in_channels=128, \n",
    "                           out_channels=128, \n",
    "                           down_flag=False)\n",
    "        \n",
    "        \n",
    "        # The fourth bottleneck\n",
    "        self.b40 = UBNeck(in_channels=128, \n",
    "                          out_channels=64, \n",
    "                          relu=True)\n",
    "        \n",
    "        self.b41 = RDDNeck(dilation=1, \n",
    "                           in_channels=64, \n",
    "                           out_channels=64, \n",
    "                           down_flag=False, \n",
    "                           relu=True)\n",
    "        \n",
    "        self.b42 = RDDNeck(dilation=1, \n",
    "                           in_channels=64, \n",
    "                           out_channels=64, \n",
    "                           down_flag=False, \n",
    "                           relu=True)\n",
    "        \n",
    "        \n",
    "        # The fifth bottleneck\n",
    "        self.b50 = UBNeck(in_channels=64, \n",
    "                          out_channels=16, \n",
    "                          relu=True)\n",
    "        \n",
    "        self.b51 = RDDNeck(dilation=1, \n",
    "                           in_channels=16, \n",
    "                           out_channels=16, \n",
    "                           down_flag=False, \n",
    "                           relu=True)\n",
    "        \n",
    "        \n",
    "        # Final ConvTranspose Layer\n",
    "        self.fullconv = nn.ConvTranspose2d(in_channels=16, \n",
    "                                           out_channels=self.C, \n",
    "                                           kernel_size=3, \n",
    "                                           stride=2, \n",
    "                                           padding=1, \n",
    "                                           output_padding=1,\n",
    "                                           bias=False)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        # The initial block\n",
    "        x = self.init(x)\n",
    "        \n",
    "        # The first bottleneck\n",
    "        x, i1 = self.b10(x)\n",
    "        x = self.b11(x)\n",
    "        x = self.b12(x)\n",
    "        x = self.b13(x)\n",
    "        x = self.b14(x)\n",
    "        \n",
    "        # The second bottleneck\n",
    "        x, i2 = self.b20(x)\n",
    "        x = self.b21(x)\n",
    "        x = self.b22(x)\n",
    "        x = self.b23(x)\n",
    "        x = self.b24(x)\n",
    "        x = self.b25(x)\n",
    "        x = self.b26(x)\n",
    "        x = self.b27(x)\n",
    "        x = self.b28(x)\n",
    "        \n",
    "        # The third bottleneck\n",
    "        x = self.b31(x)\n",
    "        x = self.b32(x)\n",
    "        x = self.b33(x)\n",
    "        x = self.b34(x)\n",
    "        x = self.b35(x)\n",
    "        x = self.b36(x)\n",
    "        x = self.b37(x)\n",
    "        x = self.b38(x)\n",
    "        \n",
    "        # The fourth bottleneck\n",
    "        x = self.b40(x, i2)\n",
    "        x = self.b41(x)\n",
    "        x = self.b42(x)\n",
    "        \n",
    "        # The fifth bottleneck\n",
    "        x = self.b50(x, i1)\n",
    "        x = self.b51(x)\n",
    "        \n",
    "        # Final ConvTranspose Layer\n",
    "        x = self.fullconv(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Instantiate the ENet model\n",
    "load_pretrained = True\n",
    "enet = ENet(2)\n",
    "if load_pretrained:\n",
    "    state_dict = torch.load('content/ckpt-enet-1000.pth')['state_dict']\n",
    "    enet.load_state_dict(state_dict)\n",
    "    print('loaded a pretrained model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking if there is any gpu available and pass the model to gpu or cpu\n",
    "device = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')\n",
    "enet = enet.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = plt.imread(df_enet['img_path'][8])\n",
    "print(img.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#build a generater\n",
    "from ImgAugumentation import ImgAugumentation\n",
    "img_augumentation = ImgAugumentation()\n",
    "def loader(df, batch_size, im_size=(512,512), aug=False):\n",
    "    total_files_s=len(df)\n",
    "    if str(batch_size).lower() == 'all':\n",
    "        batch_size = total_files_s\n",
    "    idx = 0\n",
    "    while(1):\n",
    "      # Choosing random indexes of images and labels\n",
    "        batch_idxs = np.random.randint(0, total_files_s, batch_size)\n",
    "   \n",
    "        \n",
    "        inputs = []\n",
    "        labels = []\n",
    "        \n",
    "        for jj in batch_idxs:\n",
    "          # Reading normalized photo\n",
    "        \n",
    " \n",
    "            img = plt.imread(df['img_path'][jj])\n",
    "            orginal_im_size=img.shape\n",
    "          # Resizing using nearest neighbor method\n",
    "            img = cv2.resize(img, (im_size[0], im_size[1]), cv2.INTER_NEAREST)\n",
    "            \n",
    "          # creating semantic mask \n",
    "            outer_poly=df['outer_poly'][jj] \n",
    "            #print('outer poly',outer_poly)\n",
    "            inner_poly=df['inner_poly'][jj]\n",
    "            #print('inner_poly', list(inner_poly))\n",
    "            if len(outer_poly) > 1:\n",
    "                outer_mask=get_mask((orginal_im_size[0], orginal_im_size[1]),outer_poly,display=False)\n",
    "                inner_mask=get_mask((orginal_im_size[0], orginal_im_size[1]),inner_poly,display=False)\n",
    "                inner_mask.dtype='uint8'\n",
    "                outer_mask.dtype='uint8'\n",
    "                final_mask=cv2.subtract(outer_mask,inner_mask)\n",
    "              # Resizing using nearest neighbor method\n",
    "                final_mask = cv2.resize(final_mask, (im_size[0], im_size[1]), cv2.INTER_NEAREST)\n",
    "                if aug:\n",
    "                    rand_value_augument = np.random.randint(7)\n",
    "                    if (rand_value_augument == 0):   \n",
    "                        img, final_mask=img_augumentation.flip_image_horz(img, final_mask)\n",
    "                    if (rand_value_augument == 1): \n",
    "                        img, final_mask=img_augumentation.flip_image_ver(img, final_mask)\n",
    "                    if (rand_value_augument == 2) or (rand_value_augument == 3): \n",
    "                        img=img_augumentation.brightness_images(img)\n",
    "                    if (rand_value_augument == 4): \n",
    "                        img, final_mask =img_augumentation.trans_image(img,final_mask,50)\n",
    "                    if (rand_value_augument == 5): \n",
    "                        img, final_mask =img_augumentation.stretch_image(img, final_mask,-100)\n",
    "                    if (rand_value_augument == 6): \n",
    "                        img, final_mask =img_augumentation.stretch_image(img, final_mask,-50)\n",
    "                    if (rand_value_augument == 7): \n",
    "                        img, final_mask =img_augumentation.stretch_image(img, final_mask,+30)\n",
    "                inputs.append(img)\n",
    "                labels.append(final_mask)\n",
    "\n",
    "        inputs = np.stack(inputs, axis=2)\n",
    "      # Changing image format to C x H x W\n",
    "        inputs = torch.tensor(inputs).transpose(0, 2).transpose(1, 3)\n",
    "        \n",
    "        labels = torch.tensor(labels)\n",
    "        \n",
    "        yield inputs, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#test the generater\n",
    "batch_size=10\n",
    "training_gen=loader(df_enet, batch_size=batch_size, aug=True)\n",
    "batch_img,batch_mask = next(training_gen)\n",
    "print(batch_mask[0].shape)\n",
    "\n",
    "for i in range(0,len(batch_img)):\n",
    " \n",
    "    plt.figure(figsize=(16,16))\n",
    "    plt.subplot(1,3,1)\n",
    "    plt.imshow(batch_img[i])\n",
    "    plt.axis('off')\n",
    "    plt.subplot(1,3,2)\n",
    "    plt.imshow(batch_mask[i])\n",
    "    plt.axis('off')\n",
    "\n",
    "    plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_class_weights(num_classes, c=1.02):\n",
    "    pipe = loader(df_enet, batch_size='all',aug=True)\n",
    "    _, labels = next(pipe)\n",
    "    all_labels = labels.flatten()\n",
    "    each_class = np.bincount(all_labels, minlength=num_classes)\n",
    "    prospensity_score = each_class / len(all_labels)\n",
    "    class_weights = 1 / (np.log(c + prospensity_score))\n",
    "    return class_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weights = get_class_weights(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 5e-4\n",
    "batch_size = 20\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(weight=torch.FloatTensor(class_weights).to(device))\n",
    "optimizer = torch.optim.Adam(enet.parameters(), \n",
    "                             lr=lr,\n",
    "                             weight_decay=2e-4)\n",
    "\n",
    "print_every = 20\n",
    "eval_every = 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(df_enet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#splitting the dataset into training and testing\n",
    "from sklearn.model_selection import train_test_split\n",
    "df_trn, df_tst = train_test_split(df_enet, test_size=0.03)\n",
    "print(len(df_trn))\n",
    "print(len(df_tst))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trn.reset_index(drop=True, inplace=True)\n",
    "df_tst.reset_index(drop=True, inplace=True)\n",
    "df_trn.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#create overfitting dataset\n",
    "df_trn, df_tst = train_test_split(df_tst, test_size=0.1)\n",
    "print(len(df_trn))\n",
    "print(len(df_tst))\n",
    "df_trn.reset_index(drop=True, inplace=True)\n",
    "df_tst.reset_index(drop=True, inplace=True)\n",
    "df_trn.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_losses = []\n",
    "eval_losses = []\n",
    "\n",
    "bc_train = 100# 1 // batch_size # mini_batch train\n",
    "bc_eval = 77# // batch_size  # mini_batch validation\n",
    "\n",
    "# Define pipeline objects\n",
    "pipe = loader(df_trn, batch_size,aug=True)\n",
    "eval_pipe = loader(df_tst, batch_size)\n",
    "\n",
    "epochs = 1000\n",
    "\n",
    "# Train loop\n",
    "\n",
    "for e in range(1, epochs+1):\n",
    "    \n",
    "    \n",
    "    train_loss = 0\n",
    "    print ('-'*15,'Epoch %d' % e, '-'*15)\n",
    "    \n",
    "    enet.train()\n",
    "    \n",
    "    for _ in tqdm(range(bc_train)):\n",
    "        X_batch, mask_batch = next(pipe)\n",
    "        \n",
    "        # assign data to cpu/gpu\n",
    "        X_batch, mask_batch = X_batch.to(device), mask_batch.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        out = enet(X_batch.float())\n",
    "        #print(out.shape)\n",
    "        # loss calculation\n",
    "        loss = criterion(out, mask_batch.long())\n",
    "        # update weights\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        \n",
    "    print ()\n",
    "    train_losses.append(train_loss)\n",
    "    \n",
    "    if (e+1) % print_every == 0:\n",
    "        print ('Epoch {}/{}...'.format(e, epochs),\n",
    "                'Loss {:6f}'.format(train_loss))\n",
    "    \n",
    "    if e % eval_every == 0:\n",
    "        with torch.no_grad():\n",
    "            enet.eval()\n",
    "            \n",
    "            eval_loss = 0\n",
    "\n",
    "            # Validation loop\n",
    "            for _ in tqdm(range(bc_eval)):\n",
    "                inputs, labels = next(eval_pipe)\n",
    "\n",
    "                inputs=inputs.float()\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                    \n",
    "                \n",
    "                out = enet(inputs)\n",
    "                \n",
    "                out = out.data.max(1)[1]\n",
    "                \n",
    "                eval_loss += (labels.long() - out.long()).sum()\n",
    "                \n",
    "            \n",
    "            print ()\n",
    "            print ('Loss {:6f}'.format(eval_loss))\n",
    "            \n",
    "            eval_losses.append(eval_loss)\n",
    "        \n",
    "    if e % print_every == 0:\n",
    "        checkpoint = {\n",
    "            'epochs' : e,\n",
    "            'state_dict' : enet.state_dict()\n",
    "        }\n",
    "        torch.save(checkpoint, 'content/ckpt-enet-{}-{}.pth'.format(e, train_loss))\n",
    "        print ('Model saved!')\n",
    "\n",
    "print ('Epoch {}/{}...'.format(e, epochs),\n",
    "       'Total Mean Loss: {:6f}'.format(sum(train_losses) / epochs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Postprocessing pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_img=49\n",
    "tmg_ = plt.imread(df_tst['img_path'][sample_img])\n",
    "img_size=tmg_.shape\n",
    "print(img_size)\n",
    "#tmg_ = cv2.resize(tmg_, (img_size[0], img_size[1]), cv2.INTER_NEAREST)\n",
    "tmg = torch.tensor(tmg_).unsqueeze(0).float()\n",
    "tmg = tmg.transpose(2, 3).transpose(1, 2).to(device)\n",
    "\n",
    "enet.to(device)\n",
    "with torch.no_grad():\n",
    "    out1 = enet(tmg.float()).squeeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "outer_poly=df_tst['outer_poly'][sample_img] \n",
    "#print('outer poly',outer_poly)\n",
    "inner_poly=df_tst['inner_poly'][sample_img]\n",
    "#print('inner_poly', list(inner_poly))\n",
    "outer_mask=get_mask((img_size[0], img_size[1]),outer_poly,display=False)\n",
    "inner_mask=get_mask((img_size[0], img_size[1]),inner_poly,display=False)\n",
    "inner_mask.dtype='uint8'\n",
    "outer_mask.dtype='uint8'\n",
    "final_mask=cv2.subtract(outer_mask,inner_mask)\n",
    "plt.imshow(final_mask)\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load a pretrained model if needed\n",
    "enet = ENet(2)\n",
    "state_dict = torch.load('content/ckpt-enet-108-3.242320427671075.pth')['state_dict']\n",
    "enet.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out2 = out1.cpu().detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mno = 1 # Should be between 0 - n-1 | where n is the number of classes\n",
    "\n",
    "figure = plt.figure(figsize=(20, 10))\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.title('Input Image')\n",
    "plt.axis('off')\n",
    "plt.imshow(tmg_)\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.title('Output Image')\n",
    "plt.axis('off')\n",
    "plt.imshow(out2[mno, :, :])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b_ = out1.data.max(0)[1].cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_segmap(image):\n",
    "    gate = [128, 128, 128]\n",
    "     \n",
    "\n",
    "    label_colours = np.array([gate]).astype(np.uint8)\n",
    "    r = np.zeros_like(image).astype(np.uint8)\n",
    "    g = np.zeros_like(image).astype(np.uint8)\n",
    "    b = np.zeros_like(image).astype(np.uint8)\n",
    "    for l in range(0, 1):\n",
    "        r[image == l] = label_colours[l, 0]\n",
    "        g[image == l] = label_colours[l, 1]\n",
    "        b[image == l] = label_colours[l, 2]\n",
    "\n",
    "    rgb = np.zeros((image.shape[0], image.shape[1], 3)).astype(np.uint8)\n",
    "    rgb[:, :, 0] = b\n",
    "    rgb[:, :, 1] = g\n",
    "    rgb[:, :, 2] = r\n",
    "    return rgb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_seg = decode_segmap(final_mask)\n",
    "pred_seg = decode_segmap(b_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure = plt.figure(figsize=(20, 10))\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.title('Input Image')\n",
    "plt.axis('off')\n",
    "plt.imshow(tmg_)\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.title('Predicted Segmentation')\n",
    "plt.axis('off')\n",
    "plt.imshow(pred_seg)\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.title('Ground Truth')\n",
    "plt.axis('off')\n",
    "plt.imshow(true_seg)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Post-processing pipeline      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "training_gen = generate_train_batch(cars_15_35GB,1)\n",
    "batch_img,batch_mask = next(training_gen)\n",
    "end = time.time()\n",
    "end-start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "sample_imgs=1 \n",
    "testing_gen = generate_test_batch(cars_15_35GB,sample_imgs)\n",
    "pre_final_predictions= model.predict(batch_img)\n",
    "end = time.time()\n",
    "end-start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "### Test on last frames of data\n",
    "start = time.time()\n",
    "batch_img,batch_mask = next(testing_gen)\n",
    "pre_final_predictions= model.predict(batch_img)\n",
    "np.shape(pre_final_predictions)\n",
    "for i in range(sample_imgs):\n",
    "    im=batch_img[i]\n",
    "    pred,im = next_img(im)\n",
    "    im  = np.array(im,dtype= np.uint8)\n",
    "    im_pred = np.array(255*pred[0],dtype=np.uint8)\n",
    "    im_mask = np.array(255*batch_mask[i],dtype=np.uint8)\n",
    "    rgb_mask_true= cv2.cvtColor(im_mask,cv2.COLOR_GRAY2RGB)\n",
    "    rgb_mask_true[:,:,0] = 0*rgb_mask_true[:,:,0]\n",
    "    rgb_mask_true[:,:,2] = 0*rgb_mask_true[:,:,2]\n",
    "    img_true = cv2.addWeighted(rgb_mask_true,0.70,im,0.70,0)\n",
    "    rgb_mask_pred = cv2.cvtColor(im_pred,cv2.COLOR_GRAY2RGB)\n",
    "    rgb_mask_pred[:,:,1:3] = 0*rgb_mask_pred[:,:,1:2]\n",
    "    img_pred = cv2.addWeighted(rgb_mask_pred,0.70,im,1,0)\n",
    "    draw_img = get_BB_new_img(im)\n",
    "    plt.figure(figsize=(14,5))\n",
    "    plt.subplot(1,4,1)\n",
    "    plt.imshow(im)\n",
    "    plt.title('Original')\n",
    "    plt.axis('off')\n",
    "    plt.subplot(1,4,2)\n",
    "    plt.imshow(img_pred)\n",
    "    plt.title('Segmented')\n",
    "    plt.axis('off')\n",
    "    plt.subplot(1,4,3)\n",
    "    plt.imshow(draw_img)\n",
    "    plt.title('Predicted')\n",
    "    plt.axis('off')\n",
    "    plt.subplot(1,4,4)\n",
    "    plt.imshow(img_true)\n",
    "    plt.title('Gtruth')\n",
    "    plt.axis('off')\n",
    "\n",
    "end = time.time()\n",
    "end-start   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_img = 'scene01021.jpg'\n",
    "im = cv2.imread(test_img)\n",
    "im = cv2.cvtColor(im,cv2.COLOR_BGR2RGB)\n",
    "pred,im = next_img(im)\n",
    "im  = np.array(im,dtype= np.uint8)\n",
    "im_pred = np.array(255*pred[0],dtype=np.uint8)\n",
    "rgb_mask_pred = cv2.cvtColor(im_pred,cv2.COLOR_GRAY2RGB)\n",
    "rgb_mask_pred[:,:,1:3] = 0*rgb_mask_pred[:,:,1:2]\n",
    "\n",
    "img_pred = cv2.addWeighted(rgb_mask_pred,0.70,im,1,0)\n",
    "\n",
    "draw_img = get_BB_new_img(im)\n",
    "\n",
    "plt.figure(figsize=(14,5))\n",
    "plt.subplot(1,3,1)\n",
    "plt.imshow(im)\n",
    "plt.title('Original')\n",
    "plt.axis('off')\n",
    "plt.subplot(1,3,2)\n",
    "plt.imshow(img_pred)\n",
    "plt.title('Segmentated')\n",
    "plt.axis('off')\n",
    "plt.subplot(1,3,3)\n",
    "plt.imshow(draw_img)\n",
    "plt.title('Bounding Box')\n",
    "plt.axis('off');\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performing camera calibration\n",
    "Camera calibration is performed in order to correct the deformation in the images that is caused to the optic lens curvature. The first step is to print a chessboard and take random pictures of it. Then count the chess intersecting squires to provide \"objp\" which holds the (x,y,z) coordinates of these corners. Z=0 here and the object points are the same for all images in the calibration folder. The objpoints will be appended in \"objp\" every time the method successfully detect all chessboard corners in a test image. \"imgpoints\" will be appended with the (x, y) pixel position of each of the corners in the image plane with each successful chessboard detection.\n",
    "\"objpoints\" and \"imgpoints\" were used to compute the camera calibration and distortion coefficients using the \"cv2.calibrateCamera()\" function on a test image in \"cv2.undistort()\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare object points. The number of corners are 6x9\n",
    "objp = np.zeros((6*9,3), np.float32)\n",
    "objp[:,:2] = np.mgrid[0:9, 0:6].T.reshape(-1,2)\n",
    "# Arrays to store object points and image points from all the images.\n",
    "objpoints = [] # 3d points in real world space\n",
    "imgpoints = [] # 2d points in image plane.\n",
    "# Make a list of calibration images, all located in camera_cal\n",
    "images = glob.glob('camera_cal/calibration*.jpg')\n",
    "# Step through the list and search for chessboard corners\n",
    "for idx, fname in enumerate(images):\n",
    "    img = cv2.imread(fname)\n",
    "    # imread reads images in BGR format\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    # Find the chessboard corners\n",
    "    ret, corners = cv2.findChessboardCorners(gray, (9,6), None)\n",
    "    # If found, add object points, image points\n",
    "    if ret == True:\n",
    "        objpoints.append(objp)\n",
    "        imgpoints.append(corners)\n",
    "        #Draw and display the corners\n",
    "        cv2.drawChessboardCorners(img, (9,6), corners, ret)\n",
    "        #write_name = 'corners_found'+str(idx)+'.jpg'\n",
    "        #cv2.imwrite(write_name, img)\n",
    "        #cv2.imshow('img', img)\n",
    "        #cv2.waitKey(500)\n",
    "#cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform distortion removal on test images¶\n",
    "1. Has the distortion correction been correctly applied to each image?\n",
    "Undistortion is performed on the provided test images before they are used in the pipeline. This also applies to the video frames. \"dst\" holds undistorted frames from \"cv2.undistort\" that were computed using \"mtx\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for image_name in glob.glob(\"Frames/*\"):\n",
    "    im = cv2.imread(image_name)\n",
    "    im = cv2.cvtColor(im,cv2.COLOR_BGR2RGB)\n",
    "    im = undistort(im,read=False, display = False, write = False)\n",
    "    pred,im = next_img(im)\n",
    "    im  = np.array(im,dtype= np.uint8)\n",
    "    im_pred = np.array(255*pred[0],dtype=np.uint8)\n",
    "    rgb_mask_pred = cv2.cvtColor(im_pred,cv2.COLOR_GRAY2RGB)\n",
    "    rgb_mask_pred[:,:,1:3] = 0*rgb_mask_pred[:,:,1:2]\n",
    "    img_pred = cv2.addWeighted(rgb_mask_pred,0.50,im,1,0)\n",
    "    draw_img = get_BB_new_img(im)\n",
    "    plt.figure(figsize=(14,5))\n",
    "    plt.subplot(1,3,1)\n",
    "    plt.imshow(im)\n",
    "    plt.title('Original')\n",
    "    plt.axis('off')\n",
    "    plt.subplot(1,3,2)\n",
    "    plt.imshow(img_pred)\n",
    "    plt.title('Segmentated')\n",
    "    plt.axis('off')\n",
    "    plt.subplot(1,3,3)\n",
    "    plt.imshow(draw_img)\n",
    "    plt.title('Bounding Box')\n",
    "    plt.axis('off');\n",
    "\n",
    "heatmap_10 = [np.zeros((640,960))]*10\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def video_pipeline(image):\n",
    "    #test_img = 'scene01021.jpg'\n",
    "    #im = cv2.imread(im)\n",
    "    #im = cv2.cvtColor(im,cv2.COLOR_BGR2RGB)\n",
    "    #img = get_BB_new_img(im)\n",
    "    # Apply bounding box to image\n",
    "    image_bb = np.copy(image)\n",
    "    bbox_cars = get_BB_new(image_bb)\n",
    "    img_size = np.shape(image)\n",
    "    result = image\n",
    "    img_res_shape = result.shape\n",
    "    for bbox in bbox_cars:\n",
    "        cv2.rectangle(result,(np.int32(bbox[0][0]*img_res_shape[1]/960),np.int32(bbox[0][1]*img_res_shape[0]/640)), (np.int32(bbox[1][0]*img_res_shape[1]/960),np.int32(bbox[1][1]*img_res_shape[0]/640)),(0,255,0),6)\n",
    "    #heatmap = get_Unet_mask(image_bb)\n",
    "    #plt.imshow(img)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_output = 'project_video_output.mp4'\n",
    "clip1 = VideoFileClip(\"project_video.mp4\")\n",
    "white_clip = clip1.fl_image(video_pipeline) \n",
    "white_clip.write_videofile(video_output, audio=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "HTML(\"\"\"\n",
    "<video width=\"640\" height=\"360\" controls>\n",
    "  <source src=\"{0}\">\n",
    "</video>\n",
    "\"\"\".format('project_video_output.mp4'))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:tf]",
   "language": "python",
   "name": "conda-env-tf-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
